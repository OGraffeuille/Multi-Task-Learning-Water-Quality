{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.utility.utility_data import *\n",
    "from src.utility.utility_misc import *\n",
    "from src.MTL.train_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves a dataframe of results, deletes the previous dataframe if necessary\n",
    "# Also saves numpy arrays in dict\n",
    "def save_results(results):\n",
    "    global results_df\n",
    "    \n",
    "    # Determine naming variables\n",
    "    tune_cols = [col for col in param_loader.iter_param_keys.copy() if col not in [\"seed\"]]\n",
    "\n",
    "    # Results dataframe: update the dataframe\n",
    "    results_df = results_df.append(results, ignore_index=True)\n",
    "            \n",
    "    # Save new dataframe csv if end of seed (all seeds have same number of entries)\n",
    "    seed_counts = results_df[\"seed\"].value_counts()\n",
    "    if all(seed_count == seed_counts.iloc[0] for seed_count in seed_counts):\n",
    "        \n",
    "        seeds = [int(results_df[\"seed\"].min()), int(results_df[\"seed\"].max())]\n",
    "        filename_results = r\"{}_{}_{}__{}___seeds{:d}-{:d}.{}.csv\".format(\n",
    "                params[\"algo_name\"], dataset_names, exp_name, t0.strftime(\"%y.%m.%d_%H.%M.%S\"), seeds[0], seeds[1], \".\".join(tune_cols)\n",
    "            )\n",
    "        filename_results_old = r\"{}_{}_{}__{}___seeds{:d}-{:d}.{}.csv\".format(\n",
    "                params[\"algo_name\"], dataset_names, exp_name, t0.strftime(\"%y.%m.%d_%H.%M.%S\"), seeds[0], seeds[1]-1, \".\".join(tune_cols)\n",
    "            )\n",
    "        print(\"SAVING...\\n{}\".format(filename_results))\n",
    "\n",
    "        # Delete old csv\n",
    "        if os.path.exists(r\"results/raw/\" + filename_results_old):\n",
    "            os.remove(r\"results/raw/\" + filename_results_old)\n",
    "\n",
    "        # Create new csv\n",
    "        results_df.to_csv(r\"results/raw/\" + filename_results, index=False)\n",
    "\n",
    "        print(datetime.datetime.now() - t0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Zealand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OVERALL\n",
    "exp_name = \"exps_demo\"\n",
    "dataset_names = [[\"nz\", \"gloria\"]]              # Must be nested list\n",
    "data_labels = [[\"chl\"]]                   # Must be nested list\n",
    "algo_name = \"MLP_STL\"\n",
    "\n",
    "assert isinstance(data_labels[0], list), \"ERROR: data_labels must be a nested list.\"\n",
    "assert isinstance(dataset_names[0], list), \"ERROR: dataset_names must be a nested list.\"\n",
    "assert len(dataset_names) == 1, \"ERROR: Dataset names must be length 1: exps on multiple datasets should be performed separately.\"\n",
    "\n",
    "# DATA\n",
    "data_params = {}\n",
    "data_params[\"datasets\"] = dataset_names\n",
    "data_params[\"labels\"] = data_labels\n",
    "\n",
    "data_params[\"invalid_label_action\"] = \"drop_if_all\"\n",
    "data_params[\"task_selection_criterion\"] = \"site\"\n",
    "data_params[\"split_method\"] = \"test_on_LOO_dataset\"\n",
    "data_params[\"LOO_dataset\"] = 0 # The 0th dataset in the list (NZ) is the test dataset, while the other datasets (gloria) are used for transfer learning\n",
    "data_params[\"vali_frac\"] = 0 if algo_name in [\"Naive\"] else 0.1\n",
    "data_params[\"test_frac\"] = 0.5\n",
    "\n",
    "# MODEL\n",
    "params = {}   \n",
    "\n",
    "params[\"is_network_model\"] = \"MLP\" in algo_name or \"MDN\" in algo_name\n",
    "\n",
    "params[\"verbose\"] = False\n",
    "params[\"plot\"] = False\n",
    "params[\"seed\"] = [*range(0,50)]\n",
    "params[\"num_ensembles\"] = [1, 10] if params[\"is_network_model\"] else [1]\n",
    "params[\"agg_ensembles\"] = True\n",
    "\n",
    "params[\"algo_name\"] = algo_name\n",
    "\n",
    "if params[\"is_network_model\"]:\n",
    "    params[\"lr\"] = [10**p for p in [-3, -3.5, -4]]\n",
    "    params[\"netsize\"] = 100\n",
    "    params[\"vali_epoch_freq\"] = 5\n",
    "    params[\"vali_epoch_delay\"] = 20\n",
    "    params[\"batch_shuffle\"] = True\n",
    "    params[\"max_epochs\"] = 2000   \n",
    "\n",
    "    if \"MDN\" in algo_name:\n",
    "        params[\"num_gaussians\"] = [5]\n",
    "    if \"MTL\" in algo_name:\n",
    "        params[\"netsize_lastlayer_small\"] = [False]\n",
    "        params[\"min_site_size\"] = [5]#, 10, 20, 50]\n",
    "\n",
    "if \"RF\" in algo_name:\n",
    "    params[\"rf_max_depth\"] = [None, 5, 10]\n",
    "    params[\"rf_min_samples_split\"] = [5]\n",
    "    params[\"rf_max_features\"] = ['auto', 'sqrt']\n",
    "if \"XGBoost\" in algo_name:\n",
    "    params[\"xgb_lr\"] = [0.01, 0.03, 0.1, 0.3]\n",
    "    params[\"xgb_max_depth\"] = [4, 5, 6]\n",
    "if \"SVM\" in algo_name:\n",
    "    params[\"svm_kernel\"] = ['linear', 'rbf']\n",
    "    params[\"svm_gamma\"] = [0.1, 1, 'scale']\n",
    "    params[\"svm_C\"] = [0.1, 1, 10]\n",
    "\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "# Iterate through hyperparameter combinations to find best performing model\n",
    "results_df = pd.DataFrame()\n",
    "param_loader = param_iterator(data_params, params)\n",
    "for i in range(param_loader.num_combinations):\n",
    "    d_p, p = param_loader.next()\n",
    "\n",
    "    out_dim = 3 if \"MDN\" in p[\"algo_name\"] else len(d_p[\"labels\"])\n",
    "    \n",
    "    if p[\"is_network_model\"]:\n",
    "        p[\"batch_size\"] = 16 if d_p[\"datasets\"] == [\"nz\"] else 64\n",
    "    if \"STL\" in p[\"algo_name\"]:\n",
    "        p[\"arch\"] = [[p[\"netsize\"],p[\"netsize\"],p[\"netsize\"],out_dim],[]]\n",
    "    if \"MTL\" in p[\"algo_name\"]:\n",
    "        if p[\"netsize_lastlayer_small\"]:\n",
    "            p[\"arch\"] = [[p[\"netsize\"],p[\"netsize\"],10],[out_dim]]\n",
    "        else:\n",
    "            p[\"arch\"] = [[p[\"netsize\"],p[\"netsize\"],p[\"netsize\"]],[out_dim]]\n",
    "\n",
    "    MTL = train_model(p, d_p)\n",
    "\n",
    "    results = MTL.metrics.copy()\n",
    "    for key in d_p:\n",
    "        results[key] = d_p[key]\n",
    "\n",
    "    mats_to_save_names = [f\"{y}_{label}_{partition}\" for y in [\"pred\", \"true\"] for label in d_p[\"labels\"]\n",
    "                                                        for partition in [\"train\", \"vali\", \"test\"]]\n",
    "    mats_to_save = {m: getattr(MTL, m) for m in mats_to_save_names}\n",
    "\n",
    "    save_results(results)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLORIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERALL\n",
    "exp_name = \"exps_demo\"\n",
    "dataset_names = [[\"gloria\"]]                   # Must be nested list\n",
    "data_labels = [[\"chl\"], [\"tss\"], [\"cdom\"]]     # Must be nested list\n",
    "algo_name = \"MLP_STL\"\n",
    "\n",
    "assert isinstance(data_labels[0], list), \"ERROR: data_labels must be a nested list.\"\n",
    "assert isinstance(dataset_names[0], list), \"ERROR: dataset_names must be a nested list.\"\n",
    "assert len(dataset_names) == 1, \"ERROR: Dataset names must be length 1: exps on multiple datasets should be performed separately.\"\n",
    "\n",
    "# DATA\n",
    "data_params = {}\n",
    "data_params[\"datasets\"] = dataset_names\n",
    "data_params[\"labels\"] = data_labels\n",
    "\n",
    "data_params[\"invalid_label_action\"] = \"drop_if_all\"\n",
    "data_params[\"task_selection_criterion\"] = \"site\"\n",
    "data_params[\"split_method\"] = \"random_equal_tasks\"\n",
    "data_params[\"vali_frac\"] = 0 if algo_name in [\"Naive\"] else 0.1\n",
    "data_params[\"test_frac\"] = 0.5\n",
    "data_params[\"label_ln_coefs\"] = {\"chl\": 1, \"tss\": 1, \"cdom\": 0.1}\n",
    "\n",
    "# MODEL\n",
    "params = {}   \n",
    "\n",
    "params[\"is_network_model\"] = \"MLP\" in algo_name or \"MDN\" in algo_name\n",
    "\n",
    "params[\"verbose\"] = False\n",
    "params[\"plot\"] = False\n",
    "params[\"seed\"] = [*range(0,50)]\n",
    "params[\"num_ensembles\"] = [1, 10] if params[\"is_network_model\"] else [1]\n",
    "params[\"agg_ensembles\"] = True\n",
    "\n",
    "params[\"algo_name\"] = algo_name\n",
    "\n",
    "if params[\"is_network_model\"]:\n",
    "    params[\"lr\"] = [10**p for p in [-3, -3.5, -4]]\n",
    "    params[\"netsize\"] = 100\n",
    "    params[\"vali_epoch_freq\"] = 5\n",
    "    params[\"vali_epoch_delay\"] = 20\n",
    "    params[\"batch_shuffle\"] = True\n",
    "    params[\"max_epochs\"] = 2000   \n",
    "\n",
    "    if \"MDN\" in algo_name:\n",
    "        params[\"num_gaussians\"] = [5]\n",
    "    if \"MTL\" in algo_name:\n",
    "        params[\"netsize_lastlayer_small\"] = [False]\n",
    "        params[\"min_site_size\"] = [5]#, 10, 20, 50]\n",
    "\n",
    "if \"RF\" in algo_name:\n",
    "    params[\"rf_max_depth\"] = [None, 5, 10]\n",
    "    params[\"rf_min_samples_split\"] = [5]\n",
    "    params[\"rf_max_features\"] = ['auto', 'sqrt']\n",
    "if \"XGBoost\" in algo_name:\n",
    "    params[\"xgb_lr\"] = [0.01, 0.03, 0.1, 0.3]\n",
    "    params[\"xgb_max_depth\"] = [4, 5, 6]\n",
    "if \"SVM\" in algo_name:\n",
    "    params[\"svm_kernel\"] = ['linear', 'rbf']\n",
    "    params[\"svm_gamma\"] = [0.1, 1, 'scale']\n",
    "    params[\"svm_C\"] = [0.1, 1, 10]\n",
    "\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "# Iterate through hyperparameter combinations to find best performing model\n",
    "results_df = pd.DataFrame()\n",
    "param_loader = param_iterator(data_params, params)\n",
    "for i in range(param_loader.num_combinations):\n",
    "    d_p, p = param_loader.next()\n",
    "\n",
    "    out_dim = 3 if \"MDN\" in p[\"algo_name\"] else len(d_p[\"labels\"])\n",
    "    \n",
    "    if p[\"is_network_model\"]:\n",
    "        p[\"batch_size\"] = 16 if d_p[\"datasets\"] == [\"nz\"] else 64\n",
    "        print(p[\"batch_size\"])\n",
    "    if \"STL\" in p[\"algo_name\"]:\n",
    "        p[\"arch\"] = [[p[\"netsize\"],p[\"netsize\"],p[\"netsize\"],out_dim],[]]\n",
    "    if \"MTL\" in p[\"algo_name\"]:\n",
    "        if p[\"netsize_lastlayer_small\"]:\n",
    "            p[\"arch\"] = [[p[\"netsize\"],p[\"netsize\"],10],[out_dim]]\n",
    "        else:\n",
    "            p[\"arch\"] = [[p[\"netsize\"],p[\"netsize\"],p[\"netsize\"]],[out_dim]]\n",
    "\n",
    "    MTL = train_model(p, d_p)\n",
    "\n",
    "    results = MTL.metrics.copy()\n",
    "    for key in d_p:\n",
    "        results[key] = d_p[key]\n",
    "\n",
    "    mats_to_save_names = [f\"{y}_{label}_{partition}\" for y in [\"pred\", \"true\"] for label in d_p[\"labels\"]\n",
    "                                                        for partition in [\"train\", \"vali\", \"test\"]]\n",
    "    mats_to_save = {m: getattr(MTL, m) for m in mats_to_save_names}\n",
    "\n",
    "    save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
